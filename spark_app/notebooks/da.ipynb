{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05ba79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jre-17.0.17.10-hotspot\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Spark\\spark-3.5.7-bin-hadoop3\"\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2822dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark:SparkSession = (\n",
    "    SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"KafkaStreamingNotebook\")\n",
    "    # =========================\n",
    "    # 1. ADD KAFKA CONNECTOR\n",
    "    # =========================\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    "    )\n",
    "    # =========================\n",
    "    # 2. RESOURCE – MÁY YẾU\n",
    "    # =========================\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "    .config(\"spark.executor.instances\", \"1\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"768m\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"256m\")\n",
    "\n",
    "    # # =========================\n",
    "    # # 3. STREAMING TUNE\n",
    "    # # =========================\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"2\")\n",
    "    .config(\"spark.streaming.kafka.maxRatePerPartition\", \"50\")\n",
    "\n",
    "    # # =========================\n",
    "    # # 4. STABILITY\n",
    "    # # =========================\n",
    "    # .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5efe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, BooleanType\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"type\", StringType(), True)\n",
    "    .add(\"id\", StringType(), True)\n",
    "    .add(\"videoId\", StringType(), True)\n",
    "    .add(\n",
    "        \"data\",\n",
    "        StructType()\n",
    "        .add(\"message\", StringType(),True)\n",
    "        .add(\"authorId\", StringType(), True)\n",
    "        .add(\"authorImage\", StringType(), True)\n",
    "        .add(\"authorName\", StringType(), True)\n",
    "        .add(\"timestamp\", StringType(), True)\n",
    "        .add(\"isModerator\", BooleanType(), True))\n",
    "        ,True\n",
    ")\n",
    "\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9094\")\n",
    "    .option(\"subscribe\", \"comment\")\n",
    "    .option(\"MaxOffsetsPerTrigger\", \"10000\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7468d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, current_timestamp\n",
    "\n",
    "# 1. Parse JSON và giữ lại các metadata quan trọng từ Kafka\n",
    "raw_df = streaming_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"),\n",
    "    col(\"timestamp\").alias(\"kafka_timestamp\"), # Thời gian tin nhắn vào Kafka\n",
    "    col(\"offset\"),\n",
    "    col(\"partition\")\n",
    ")\n",
    "\n",
    "# 2. Flatten và xử lý lỗi\n",
    "processed_df = raw_df.select(\"data.*\", \"kafka_timestamp\", \"offset\", \"partition\") \\\n",
    "    .filter(\"id IS NOT NULL\") # Ví dụ: Loại bỏ các bản ghi không parse được (rác)\n",
    "\n",
    "# 3. Thêm Processing Timestamp (để đo độ trễ hệ thống - Observability)\n",
    "final_df = processed_df.withColumn(\"processing_time\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d181672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- videoId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      " |    |-- authorId: string (nullable = true)\n",
      " |    |-- authorImage: string (nullable = true)\n",
      " |    |-- authorName: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |    |-- isModerator: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aec96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = final_df.selectExpr(\"type\", \"id\", \"videoId\", \"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dab02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# Định nghĩa hàm chuẩn hóa bằng Python (chạy trên Worker)\n",
    "@pandas_udf(\"string\")\n",
    "def unicode_normalize_udf(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(lambda x: unicodedata.normalize('NFC', x) if x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485491e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, lower, col, regexp_replace, when, length, expr\n",
    "\n",
    "# 1. Định nghĩa Patterns\n",
    "url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "email_pattern = r\"\\S+@\\S+\"\n",
    "special_char_pattern = r\"(?<=[a-zA-Z])([^\\w\\s])(?=[a-zA-Z])\"\n",
    "from pyspark.sql.functions import trim, lower, col, regexp_replace, when, length\n",
    "\n",
    "# Pipeline tối ưu hóa hiệu năng\n",
    "df_comment_cleaned = df_exploded \\\n",
    "    .dropna(subset=[\"message\"]) \\\n",
    "    .withColumn(\"message_clean\", unicode_normalize_udf(col(\"message\"))) \\\n",
    "    .withColumn(\n",
    "        \"message_clean\",\n",
    "        trim(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    regexp_replace(\n",
    "                        regexp_replace(\n",
    "                            lower(col(\"message_clean\")), \n",
    "                            url_pattern, \"\"\n",
    "                        ), \n",
    "                        email_pattern, \"\"\n",
    "                    ),\n",
    "                special_char_pattern, \" \"\n",
    "                ),\n",
    "                r\"\\s+\", \" \" # Squish khoảng trắng\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import col, lit, struct, to_json\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# 1. Cấu hình HTTP Session với Retry (Global object trên mỗi Executor)\n",
    "def get_http_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "def model_inference_api(batch_df, batch_id):\n",
    "    # Dùng broadcast variable cho URL nếu cần\n",
    "    MODEL_URL = \"https://api.model-serving.local/v1/predict\"\n",
    "    TIMEOUT = 2.0  # Seconds (Phải nhỏ hơn Batch Interval)\n",
    "    \n",
    "    # Extract features thành JSON để call API\n",
    "    records = batch_df.select(\"id\", \"model_input\").collect()\n",
    "    \n",
    "    if not records:\n",
    "        return\n",
    "\n",
    "    payload = [\n",
    "        {\"id\": row.id, \"features\": row.model_input.feature_vector.toArray().tolist()} \n",
    "        for row in records\n",
    "    ]\n",
    "    \n",
    "    session = get_http_session()\n",
    "    try:\n",
    "        response = session.post(MODEL_URL, json=payload, timeout=TIMEOUT)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json() # List of {id, score, label}\n",
    "            # Join results back to processed_df (pseudo-code, normally done via another DataFrame join)\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request Failed: {e}\")\n",
    "        # Send to DLQ\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "# =========================================\n",
    "# 5. FAN-OUT SINKS (ForeachBatch Implementation)\n",
    "# =========================================\n",
    "\n",
    "def write_to_postgres(df, epoch_id):\n",
    "    # JDBC Properties\n",
    "    jdbc_url = \"jdbc:postgresql://postgres:5432/warehouse\"\n",
    "    props = {\n",
    "        \"user\": \"user\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    # Write Mode: Append (Upsert handled via temporary table in real prod)\n",
    "    df.select(\"id\", \"message_clean\", \"timestamp\", \"label\") \\\n",
    "        .write.jdbc(url=jdbc_url, table=\"comments\", mode=\"append\", properties=props)\n",
    "\n",
    "def write_to_elasticsearch(df, epoch_id):\n",
    "    # ES Config\n",
    "    es_conf = {\n",
    "        \"es.nodes\": \"elasticsearch\",\n",
    "        \"es.port\": \"9200\",\n",
    "        \"es.resource\": \"comments/_doc\",\n",
    "        \"es.mapping.id\": \"id\"\n",
    "    }\n",
    "    df.select(\"id\", \"message_clean\", \"label\") \\\n",
    "        .write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .options(**es_conf) \\\n",
    "        .mode(\"append\").save()\n",
    "\n",
    "def write_to_clickhouse(df, epoch_id):\n",
    "    # ClickHouse JDBC\n",
    "    jdbc_url = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "    props = {\n",
    "        \"user\": \"default\",\n",
    "        \"password\": \"\",\n",
    "        \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "    }\n",
    "    df.select(\"id\", \"message_clean\", \"label\") \\\n",
    "        .write.jdbc(url=jdbc_url, table=\"comments_analytics\", mode=\"append\", properties=props)\n",
    "\n",
    "def write_to_s3(df, epoch_id):\n",
    "    # Data Lake\n",
    "    df.write.mode(\"append\").partitionBy(\"timestamp\").parquet(\"s3a://datalake/comments/\")\n",
    "\n",
    "def master_process_batch(batch_df, epoch_id):\n",
    "    # Cache batch for multiple sinks\n",
    "    batch_df.persist()\n",
    "    \n",
    "    # 1. Model Inference\n",
    "    # model_inference_api(batch_df) --> In non-streaming/simulation, we might skip or mock this\n",
    "    \n",
    "    # 2. Fan-out\n",
    "    print(f\"Processing Batch {epoch_id}, Size: {batch_df.count()}\")\n",
    "    \n",
    "    # Parallelize these in production, here sequential for safety\n",
    "    try:\n",
    "        write_to_postgres(batch_df, epoch_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Postgres Error: {e}\")\n",
    "        \n",
    "    try:\n",
    "        write_to_elasticsearch(batch_df, epoch_id)\n",
    "    except Exception as e:\n",
    "        print(f\"ES Error: {e}\")\n",
    "        \n",
    "    try:\n",
    "        write_to_clickhouse(batch_df, epoch_id)\n",
    "    except Exception as e:\n",
    "        print(f\"ClickHouse Error: {e}\")\n",
    "        \n",
    "    try:\n",
    "        write_to_s3(batch_df, epoch_id)\n",
    "    except Exception as e:\n",
    "        print(f\"S3 Error: {e}\")\n",
    "\n",
    "    batch_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a653290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"console\") # Hoặc \"delta\", \"parquet\"\n",
    "    .option(\"checkpointLocation\", \"s3://my-bucket/checkpoints/comment_stream/\") # Quan trọng nhất\n",
    "    .trigger(processingTime='10 seconds') # Sử dụng Micro-batch mỗi 10s\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
